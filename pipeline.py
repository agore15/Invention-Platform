import os
import sys
import glob
from typing import List

# Add project root to sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

from crawler.agent import ThreeGPPCrawler
from brain.indexer import ChunkingStrategy
from brain.vectorizer import EmbeddingGenerator, Indexer

def run_pipeline(download_limit: int = 50):
    print('=== Starting 3GPP Data Pipeline ===')

    # 1. Crawler
    print('\n[Phase 1] Running Crawler...')
    crawler = ThreeGPPCrawler()
    # Trigger the full crawl list defined in agent.py
    crawler.download_directory()

    # 2. Refinery (Parser/Loader)
    print('\n[Phase 2] Running Refinery (Loader)...')
    chunker = ChunkingStrategy()
    
    # Look for text files generated by the crawler
    source_dir = 'data/specs'
    files = glob.glob(os.path.join(source_dir, '*.txt'))
    
    if not files:
        print('No text files found to process. Crawler might have failed.')
        return

    print(f'Found {len(files)} specification files.')
    
    all_chunks = []
    
    for file_path in files:
        print(f'  Processing {os.path.basename(file_path)}...')
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            # Create a document structure compatible with chunker
            doc_data = {
                'content': text,
                'metadata': {
                    'source': f'3GPP TS {os.path.splitext(os.path.basename(file_path))[0]}',
                    'type': 'TS',
                    'title': 'Technical Specification',
                    'status': 'Active'
                }
            }
            
            # Chunk
            doc_chunks = chunker.chunk_document(doc_data)
            all_chunks.extend(doc_chunks)
            print(f'    -> Extracted {len(doc_chunks)} chunks.')
            
        except Exception as e:
            print(f'    Error processing {file_path}: {e}')

    # 3. Brain (Embedding & Indexing)
    print(f'\n[Phase 3] Running Brain (Embedding & Indexing)...')
    print(f'Total chunks to index: {len(all_chunks)}')
    
    if all_chunks:
        embedder = EmbeddingGenerator()
        indexer = Indexer() # Defaults to brain/index.json
        
        print('  Generating embeddings...')
        vectorized_chunks = embedder.generate_embeddings(all_chunks)
        
        print('  Indexing...')
        indexer.upload_vectors(vectorized_chunks)
        
    print('\n=== Pipeline Complete ===')
    print('You can now run the UI to search this data.')

if __name__ == '__main__':
    run_pipeline()
